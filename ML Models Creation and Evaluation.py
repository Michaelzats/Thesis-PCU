# -*- coding: utf-8 -*-
"""THESIS PCU PROJECT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d_RZR8xhiVBPSOCMJKwHvPtWBXE6nAVQ

** using Machine Learning and Deep Learning Techniques**
For this study, we sourced two precleaned datasets, ensuring that the data is immediately ready for further processing and analysis available on Kaggle. The first dataset was [“Dataset Text Document Classification”](https://www.kaggle.com/datasets/jensenbaxter/10dataset-text-document-classification) published by Baxter in 2021. It comprises a collection of 1000 newsgroup documents from 10 different newsgroups categorized under ten distinct labels: ’business’, ’entertainment’, ’food’, ’graphics’, ’historical’, ’medical’, ’politics’, ’space’, ’sport’, and ’technology’. Each category is represented by 100 text files, culminating in a comprehensive dataset of 1,000 documents. The size of  the dataset was 1.8MB. Within the research it will be referred as DATASET 1.

The second dataset was chosen to be [“BBC Full Text Document Classification”](https://www.kaggle.com/datasets/shivamkushwaha/bbc-full-text-document-classification) (Kushwaha, 2019). It comprises 2225 documents from BBC news channel and categorized under five distinct labels:  ’business’, ’entertainment’,  ’politics’, ’sport’, and ’tech’. The data categorises varies with the amount of data inside, with 510 files for ’business’, 386 for ’entertainment’, 417 for ’politics’, 511 for ’sport’, and 401 files for ’tech’. As to make sure that it differentiates itself, the names of the categories were pre-renamed to ‘’business_bbc’,  ’entertainment_bbc’,  ’politics_bbc’, ’sport_bbc’, and ’tech_bbc’. Moreover, as the original file had two folders: bbc-fulltext (document classification) and bbc, bbc-fulltext (document classification) folder was deleted. Finally inside of the bbc folder, README file was deleted as well. The size of  the dataset was 3.7MB. Within the research it will be referred as DATASET 2.

For the purpose of training and evaluation, the dataset was partitioned into a 70-30 split, with 70% allocated for training and the remaining 30% reserved for testing.

The project is executed inside the payed version of Google Colab environment, google colab pro, as in order to ran the project within various of datasets it was needed to have a higher RAM (Random-access memory) as the free version gave not sufficient enough for such size project size of 12 GB. The datasets were chosen for the project execution were the secondary data from the website Kaggle, (10)Dataset Text Document Classification, and BBC Full Text Document Classification. Another point to mention, is that due to the amount of data difference, the computation of BBC Full Text Document Classification took twice as much time as from (10)Dataset Text Document Classification.

Our investigative approach encompassed a range of machine learning models, namely Naive Bayes, Support Vector Machines (SVM), Decision Tree and Random Forest. Additionally, a Deep Neural Network *(due to this model, the chanks of codes might take a while to load)*, a paradigm of deep learning, was also employed. To enhance the performance and robustness of these models, various techniques were integrated into the pipeline:

Feature Engineering: Utilization of N-grams to capture local word order information.
Word Embeddings: Leveraging pre-trained embeddings to represent words in a dense vector space.
Feature Selection: The Chi-Squared Test was employed to select significant features.
Model Ensembling: Bagging was used to reduce variance by training multiple models.
Regularization Techniques: Dropout was introduced to prevent overfitting in the deep learning model.
The models were evaluated on multiple metrics, including Precision, Recall, F1-Score, and Accuracy. The F1-Score, which harmoniously balances Precision and Recall, was chosen as the primary metric to determine the best model.

As a result, while experimenting with multiple enhancement methods, the best result was achieved with Chi-Squared Test Deep Neural Network feature selection in DATASET 1 and with Dropout Deep Neural Network, yielding an F1 Score of 0.979982 and 0.985023 respectevely. However, it’s note- worthy that Deep Neural Network is computationally intensive, that is why it was decided to take the second best performing ML model, which was SVM.

Despite the fact that it was seen the performance ehcnment with the enchnemnt tqcniques for SVM, but the difference was not confirmed by statistical confirmation based on ANOVA test, that is why SVM without enhcnment tecniqeues was taken as the best model with the results of 0.976682 for DATASET 1 and with 0.974587 for DATASET 2 based on F1-Score. The computational overhead, coupled with the less performance over the standard SVM, made the latter a more pragmatic choice for deployment. Nontherless, it must be mentioned that all the models performed well, apart from Descion Tree, which was found not to be applicable for such problem as documents classification.

Moreover, while enhancement methods can potentially boost the performance of machine learning models, it’s imperative to weigh the benefits against the computational costs. In this study, the standard SVM, with its stellar performance and efficiency, was deemed the most suitable model for the task at hand.

In conclusion, the findings from our investigation reveal that the Support Vector Machine (SVM) machine learning model, even without the application of enhancement techniques, stands out for its computational efficiency and robust average performance metrics. In light of these results, we opted for a merged dataset approach, amalgamating data from DATASET 1 and DATASET 2. This strategy not only enriches our training corpus with a diverse array of texts spanning business, entertainment, politics, sports, and technology but also closely aligns with our client's requirements for the final application deployment. Consequently, this section delineates the process for extracting and externalizing the SVM model, thereby facilitating its integration and application in the deployment of our app.

The development and refinement of our approach were greatly aided by resources from StackOverflow, ChatGPT, and a series of YouTube tutorials:

[Python Machine Learning #4 - Support Vector Machines
](https://www.youtube.com/watch?v=99Eyw7Quacc)

[Neural Network Python | How to make a Neural Network in Python | Python Tutorial | Edureka](https://www.youtube.com/watch?v=9UBqkUJVP4g)

[Machine Learning Tutorial Python - 11 Random Forest
](https://www.youtube.com/watch?v=ok2s1vV9XW0)

[Decision Tree Classification in Python (from scratch!)
](https://www.youtube.com/watch?v=sgQAhG5Q7iY)

[Naive Bayes Classifier in Python (from scratch!)
](https://www.youtube.com/watch?v=3I8oX3OUL6I)

**Literature used for deeper understanding of the models:**
A. Liaw and M. Wiener, randomForest: Breiman and Cutler’s Random Forests for Classification and Regression, 2015, r package version 4.
A. McCallum and K. Nigam, “A comparison of event models for naive bayes text classification,” pp. 41–48, 1998.
Aha, D.W., Kibler, D., and Albert, M.K., 1991. Instance-based learning algorithms. Machine learning, 6, pp.37-66.
Bengio, Y., Courville, A., and Vincent, P., 2013. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8), pp.1798-1828.
Breiman, L., 1996. Bagging predictors. Machine learning, 24, pp.123-140.
Breiman, L., 2001. Random forests. Machine learning, 45, pp.5-32.
Burges, C.J., 1998. A tutorial on support vector machines for pattern recognition. Data mining and knowledge discovery, 2(2), pp.121-167.
Chawla, N.V., Bowyer, K.W., Hall, L.O., and Kegelmeyer, W.P., 2002. SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16, pp.321-357.
Cho, K., et al., 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.
Cortes, C. and Vapnik, V., 1995. Support-vector networks. Machine learning, 20, pp.273-297.
Cover, T. and Hart, P., 1967. Nearest neighbor pattern classification. IEEE transactions on information theory, 13(1), pp.21-27.
Devlin, J., et al., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Dietterich, T.G., 2000, June. Ensemble methods in machine learning. In International workshop on multiple classifier systems (pp. 1-15). Berlin, Heidelberg: Springer Berlin Heidelberg.
Goldberg, Y. and Levy, O., 2014. word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method. arXiv preprint arXiv:1402.3722.
Goodfellow, I., Bengio, Y., and Courville, A., 2016. Deep learning. MIT press.
Hastie, T., Tibshirani, R., Friedman, J.H., and Friedman, J.H., 2009. The elements of statistical learning: data mining, inference, and prediction (Vol. 2, pp. 1-758). New York: springer.
He, H. and Garcia, E.A., 2009. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9), pp.1263-1284.
Hochreiter, S. and Schmidhuber, J., 1997. Long short-term memory. Neural computation, 9(8), pp.1735-1780.
Kim, Y., 2014. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.
Le, Q. and Mikolov, T., 2014, June. Distributed representations of sentences and documents. In International conference on machine learning (pp. 1188-1196). PMLR.
LeCun, Y., Bengio, Y., and Hinton, G., 2015. Deep learning. nature, 521(7553), pp.436-444.
Liaw, A. and Wiener, M., 2002. Classification and regression by randomForest. R news, 2(3), pp.18-22.
Mikolov, T., et al., 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.
Mikolov, T., et al., 2013. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26.
Pennington, J., Socher, R., and Manning, C.D., 2014, October. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).
Srivastava, N., et al., 2014. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1), pp.1929-1958.
Vaswani, A., et al., 2017. Attention is all you need. Advances in neural information processing systems, 30.

# 0. Pre-work Stage

Installing the required libraries
"""

# Install necessary libraries in one go
!pip install numpy pandas statsmodels scikit-learn matplotlib seaborn nltk gensim tensorflow keras==2.12.0

"""Importing Libraries and API

"""

# Import necessary libraries and API
import numpy as np
import pandas as pd
import os
from pathlib import Path
import scipy.stats as stats
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import nltk
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
import gensim.downloader as api
from gensim.models import Word2Vec
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.ensemble import BaggingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.metrics import f1_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.preprocessing import MinMaxScaler
import pickle
from google.colab import files
word2vec_model = api.load("word2vec-google-news-300")

"""# **1. Expiriement with (10)Dataset Text Document Classification Dataset (DATASET 1)**

## 1.1 Data Uploading and Data Preprocessing

Firstly it is necessary to upload the file in zip where many different folders with different categories of files are holded, so to unpack it for the further experiments.
"""

# Once uploaded, unzip using:
!unzip DATASET_10.zip
!ls

"""Following the initual unzipping process, it is extracted each category, and setting 30% of data for training and 70% for testing."""

# Define the base path where the 'bbc' folder is located
# Make sure this path points directly to the folder containing the category folders.
base_path = Path('DATASET_10')  # Replace with the correct path to your 'bbc' folder

# Dynamically list categories based on the directories in the 'bbc' folder
categories = [f.name for f in base_path.iterdir() if f.is_dir()]

texts = []
labels = []

print(f"Found categories: {categories}")

# Iterate over the categories and read the files
for category in categories:
    category_path = base_path / category
    # Print the current category path for debugging
    print(f"Reading files from category: {category}")

    for file_path in category_path.iterdir():
        if file_path.is_file() and not file_path.name.startswith('.'):
            # Print each file path being read for debugging
            print(f"Reading file: {file_path}")

            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                texts.append(file.read())
                labels.append(category)

# Check if we have collected any texts and labels
if not texts or not labels:
    raise ValueError("No texts or labels have been collected. Please check your directory paths and file permissions.")

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)

"""##1.2. ML Models Creation and Execution of the Experiment

###**Naive Bayes, SVM, Random Forest, Decision Tree, Deep Neural Network (No enhancment tecniques)**

**Description:**

This segment of code is meticulously designed to refine the process of classifying documents by integrating the concept of bigrams (sequential pairs of words) and conducting a thorough evaluation of a variety of machine learning algorithms, including an advanced deep neural network.

**Bigram Incorporation:**
Utilizing the TfidfVectorizer with the ngram_range parameter configured to (1,1). This is the standard way, and so considered to be without any enhancements.For seamless integration with Keras, the datasets designated for training and testing are transformed through this vectorization method and subsequently converted into array formats.


**Deep Neural Network Definition:**

Employing the Keras toolkit, a feed-forward neural network is constructed featuring two intermediary layers enhanced with dropout regularization to mitigate the risk of overfitting. The terminal layer is activated using a softmax function, making it apt for addressing multi-category classification challenges. The chosen loss function is sparse_categorical_crossentropy, deemed suitable for dealing with class labels encoded as integers.


**Model Training and Evaluation:**

The study delineates four distinct models for evaluation: Naive Bayes, Support Vector Machine (SVM), Random Forest, Decision Tree and the deep neural network as previously described. Each model undergoes training utilizing the training dataset and is subsequently assessed on the test dataset.

The evaluation of each model's efficacy is conducted through a comprehensive set of metrics, including precision, recall, F1-score, accuracy, and support, offering a holistic view of the performance across both positive and negative classifications.

The outcomes of these evaluations are methodically compiled and exhibited in a tabular arrangement via a DataFrame.

**Outcome:**

Executing this code will reveal the performance metrics for each model, highlighting the advantage of incorporating bigrams (1,1) in the feature extraction process. Besides, that results will be holded as the benchmarks for the future experiments with DATASET 1.
"""

# Using only unigrams
tfidf_vectorizer_ngrams = TfidfVectorizer(stop_words='english', ngram_range=(1,1))
X_train_tfidf_ngrams = tfidf_vectorizer_ngrams.fit_transform(X_train).toarray()  # Convert to array for Keras
X_test_tfidf_ngrams = tfidf_vectorizer_ngrams.transform(X_test).toarray()

# Define a simple feed-forward neural network for Keras
def create_nn_model():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train_tfidf_ngrams.shape[1], activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def train_and_evaluate_models(X_train, X_test):
    # Define the models
    models = {
        'Naive Bayes': MultinomialNB(),
        'SVM': SVC(kernel='linear'),
        'Random Forest': RandomForestClassifier(random_state=42),
        'Decision Tree': DecisionTreeClassifier(random_state=42),
        'Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)
    }

    # Train and evaluate the models
    results = {}
    for model_name, model in models.items():
        # Train
        model.fit(X_train, y_train)
        # Predict
        y_pred = model.predict(X_test)
        # Evaluate
        report = classification_report(y_test, y_pred, output_dict=True)
        results[model_name] = {
            'Precision': report['weighted avg']['precision'],
            'Recall': report['weighted avg']['recall'],
            'F1-Score': report['weighted avg']['f1-score'],
            'Accuracy': report['accuracy'],
            'Support': report['weighted avg']['support']
        }

    # Convert results to DataFrame for display
    df_results = pd.DataFrame(results).transpose()
    return df_results

# Train and evaluate models with Unigrams
df_results = train_and_evaluate_models(X_train_tfidf_ngrams, X_test_tfidf_ngrams)
print(df_results)

"""###**Naive Bayes, SVM, Random Forest, Decision Tree, Deep Neural Network (Feature Engineering - N-grams)**

**Description:**

This code segment focuses on advancing document classification techniques, with a special emphasis on the utilization of bigrams (consecutive word pairs) as critical features. It conducts a thorough evaluation of a variety of machine learning methodologies, including an intricate deep neural network.

**Bigram Feature Extraction:**

Through the application of the TfidfVectorizer, configured with an ngram_range parameter of (1,2), this technique embraces a comprehensive vectorization strategy. It accounts for both singular words (unigrams) and their adjacent couplings (bigrams), aiming to seize a deeper contextual understanding beyond the scope of unigrams alone. The datasets designated for training and testing are processed with this advanced vectorization, then converted into array formats to ensure they mesh seamlessly with the Keras environment.

**Deep Neural Network Architecture:**

Leveraging the capabilities of the Keras framework, a sophisticated feed-forward neural network is crafted. This network features two internal layers, punctuated by dropout layers to counteract overfitting effectively. Its output layer employs a softmax activation function, ideally suited for the complexities of multi-class classification tasks. The selection of sparse_categorical_crossentropy as the loss function aligns perfectly with the needs of integer-encoded class labels.

**Model Training & Evaluation Framework:**

A quartet of models is elaborated upon: Naive Bayes, Support Vector Machine (SVM), Random Forest, Decision Tree and the deep neural network as previously articulated. Training these models with the provided dataset precedes a meticulous evaluation on the testing dataset.

To assess the performance of each model accurately, a suite of metrics—precision, recall, F1-score, accuracy, and support—is utilized. These measures afford a detailed perspective on the models' effectiveness, covering both positive and negative class predictions. The synthesized results are systematically organized and displayed in a DataFrame, offering a clear, tabular overview.

**Outcome:**

The execution of this code is poised to unveil the performance metrics for each model, underlining the significant benefits of integrating bigrams into the feature extraction phase. This demonstration aims to highlight the value of incorporating richer contextual data from text, potentially elevating the efficacy of document classification endeavors.
"""

# Using bigrams
tfidf_vectorizer_ngrams = TfidfVectorizer(stop_words='english', ngram_range=(1,2))
X_train_tfidf_ngrams = tfidf_vectorizer_ngrams.fit_transform(X_train).toarray()  # Convert to array for Keras
X_test_tfidf_ngrams = tfidf_vectorizer_ngrams.transform(X_test).toarray()

# Define a simple feed-forward neural network for Keras
def create_nn_model():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train_tfidf_ngrams.shape[1], activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def train_and_evaluate_models(X_train, X_test):
    # Define the models
    models = {
        'N-grams Naive Bayes': MultinomialNB(),
        'N-grams SVM': SVC(kernel='linear'),
        'N-grams Random Forest': RandomForestClassifier(random_state=42),
        'N-grams Decision Tree': DecisionTreeClassifier(random_state=42),
        'N-grams Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)
    }

    # Train and evaluate the models
    results = {}
    for model_name, model in models.items():
        # Train
        model.fit(X_train, y_train)
        # Predict
        y_pred = model.predict(X_test)
        # Evaluate
        report = classification_report(y_test, y_pred, output_dict=True)
        results[model_name] = {
            'Precision': report['weighted avg']['precision'],
            'Recall': report['weighted avg']['recall'],
            'F1-Score': report['weighted avg']['f1-score'],
            'Accuracy': report['accuracy'],
            'Support': report['weighted avg']['support']
        }

    # Convert results to DataFrame for display
    df_results = pd.DataFrame(results).transpose()
    return df_results

# Train and evaluate models with N-grams
df_results_ngrams = train_and_evaluate_models(X_train_tfidf_ngrams, X_test_tfidf_ngrams)
print(df_results_ngrams)

"""###**Naive Bayes, SVM, Random Forest, Decision Tree, Deep Neural Network (Word Embeddings - Pre-trained Embeddings)**

**Description:**

This code segment is ingeniously crafted for the purpose of document classification, harnessing the sophisticated capabilities of Word2Vec embeddings. Word2Vec, a cutting-edge pre-trained model, excels in capturing the semantic intricacies between words by mapping them as vectors within a vast, high-dimensional space. The effectiveness of a variety of machine learning models, including a sophisticated deep neural network, is meticulously evaluated using these semantic embeddings.

**Word2Vec Embeddings:**

The renowned Google News Word2Vec model, distinguished by its training on an extensive corpus and boasting a vector size of 300, is adeptly loaded.
A specialized function, document_to_word2vec, is crafted to transform a document into its Word2Vec vector representation by averaging the vectors of the words it contains. This transformation process is applied to both the training and testing datasets, converting them into Word2Vec representations for further processing.

**Deep Neural Network Architecture:**

Utilizing the versatile Keras library, a robust feed-forward neural network is developed, featuring two hidden layers. These layers are interspersed with dropout layers to effectively curtail the risk of overfitting. The activation function selected for the output layer is softmax, rendering it ideal for multi-class classification endeavors. The loss function of choice, sparse_categorical_crossentropy, is particularly well-suited for handling integer-encoded class labels.


**Model Training & Evaluation Framework:**

The evaluation framework encompasses four distinct models: Naive Bayes (specifically GaussianNB, tailored for the continuous nature of Word2Vec features), Support Vector Machine (SVM), Random Forest, Decision Tree and the aforementioned Deep Neural Network.

Training is conducted on the datasets transformed via Word2Vec, followed by a thorough evaluation on the test dataset.To comprehensively assess the performance of each model, a variety of metrics including precision, recall, F1-score, accuracy, and support are utilized. These metrics offer a well-rounded perspective on each model's performance, accounting for both positive and negative class outcomes.The evaluation results are methodically compiled and displayed in a structured DataFrame.

**Outcome:**
Upon execution, this code segment is designed to unveil the performance metrics for each evaluated model, leveraging the sophisticated Word2Vec embeddings as features. This initiative aims to highlight the significant benefits of integrating semantic understanding into the textual data analysis, thereby enhancing the document classification process.
"""

def document_to_word2vec(doc, model):
    # Tokenize the document, filter out words not in the model's vocabulary
    words = [word for word in doc.split() if word in model.key_to_index]
    if len(words) == 0:
        return np.zeros(model.vector_size)
    # Convert words to vectors and average them
    return np.mean([model[word] for word in words], axis=0)

X_train_word2vec = np.array([document_to_word2vec(doc, word2vec_model) for doc in X_train])
X_test_word2vec = np.array([document_to_word2vec(doc, word2vec_model) for doc in X_test])

# Define a simple feed-forward neural network for Keras
def create_nn_model():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train_word2vec.shape[1], activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def train_and_evaluate_models(X_train, X_test):
    # Define the models
    models = {
        'Pre-trained Embeddings Naive Bayes': GaussianNB(),
        'Pre-trained Embeddings SVM': SVC(kernel='linear'),
        'Pre-trained Embeddings Random Forest': RandomForestClassifier(random_state=42),
        'Pre-trained Embeddings Decision Tree': DecisionTreeClassifier(random_state=42),
        'Pre-trained Embeddings Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)
    }

    # Train and evaluate the models
    results = {}
    for model_name, model in models.items():
        # Train
        model.fit(X_train, y_train)
        # Predict
        y_pred = model.predict(X_test)
        # Evaluate
        report = classification_report(y_test, y_pred, output_dict=True)
        results[model_name] = {
            'Precision': report['weighted avg']['precision'],
            'Recall': report['weighted avg']['recall'],
            'F1-Score': report['weighted avg']['f1-score'],
            'Accuracy': report['accuracy'],
            'Support': report['weighted avg']['support']
        }

    # Convert results to DataFrame for display
    df_results = pd.DataFrame(results).transpose()
    return df_results

# Train and evaluate models with Word2Vec features
df_results_word2vec = train_and_evaluate_models(X_train_word2vec, X_test_word2vec)
print(df_results_word2vec)

"""###**Naive Bayes, SVM, Random Forest, Decision Tree, Deep Neural Network (Feature Selection - Chi-Squared Test)**

**Description:**

This code segment is engineered to advance document classification efforts by integrating bigrams and utilizing the chi-squared test for feature selection. The aim is to explore how these methodologies affect the performance of a variety of machine learning models, including an advanced deep neural network.

**Bigrams with TF-IDF:**

Employing the TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer, this approach is designed to acknowledge both single words (unigrams) and word pairs (bigrams), enabling the extraction of richer contextual insights that could enhance model comprehension of textual content. The datasets for training and testing are accordingly transformed into TF-IDF representations and then arrayed to ensure seamless integration with the Keras framework.

**Feature Selection using Chi-Squared Test:**

The chi-squared test, a method to assess the independence of two categorical variables, is applied to pinpoint the top 10,000 features demonstrating the most significant associations with the target variable. This process refines the TF-IDF transformed datasets by concentrating on these key features, potentially bolstering model performance by focusing on the most relevant data.

**Deep Neural Network Architecture:**

A sophisticated feed-forward neural network is established using Keras, featuring two intermediate layers accompanied by dropout mechanisms to curtail overfitting risks. The network employs a softmax activation function for the output layer, aligning with the demands of multi-class classification tasks. The loss function selected, sparse_categorical_crossentropy, is tailored for handling integer-encoded target labels.

**Model Training & Evaluation Framework:**

The approach delineates four specific models for assessment: Naive Bayes, Support Vector Machine (SVM), Random Forest, Decision Tree and the deep neural network previously outlined. Each model is trained with the chi-squared test refined features from the training dataset and evaluated against the test dataset.

The effectiveness of each model is measured using key performance metrics such as precision, recall, F1-score, accuracy, and support. These metrics offer a rounded evaluation of performance across both the positive and negative classification outcomes. The findings are systematically assembled and displayed in a DataFrame for easy comparison.

**Outcome:**
Executing this segment of code will showcase the performance metrics for each model, highlighting the efficacy of employing bigrams alongside chi-squared feature selection. This execution is intended to illuminate the benefits of these sophisticated techniques in deepening the model's grasp of textual nuances.
"""

# Using bigrams
tfidf_vectorizer_ngrams = TfidfVectorizer(stop_words='english', ngram_range=(1,2))
X_train_tfidf_ngrams = tfidf_vectorizer_ngrams.fit_transform(X_train).toarray()  # Convert to array for Keras
X_test_tfidf_ngrams = tfidf_vectorizer_ngrams.transform(X_test).toarray()

# Select top 10,000 features based on the chi-squared test
k_best = 10000
ch2 = SelectKBest(chi2, k=k_best)
X_train_chi2_selected = ch2.fit_transform(X_train_tfidf_ngrams, y_train)
X_test_chi2_selected = ch2.transform(X_test_tfidf_ngrams)

# Define a simple feed-forward neural network for Keras
def create_nn_model():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train_chi2_selected.shape[1], activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def train_and_evaluate_models(X_train, X_test):
    # Define the models
    models = {
        'Chi-Squared Test Naive Bayes': MultinomialNB(),
        'Chi-Squared Test SVM': SVC(kernel='linear'),
        'Chi-Squared Test Random Forest': RandomForestClassifier(random_state=42),
        'Chi-Squared Test Decision Tree': DecisionTreeClassifier(random_state=42),
        'Chi-Squared Test Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)
    }

    # Train and evaluate the models
    results = {}
    for model_name, model in models.items():
        # Train
        model.fit(X_train, y_train)
        # Predict
        y_pred = model.predict(X_test)
        # Evaluate
        report = classification_report(y_test, y_pred, output_dict=True)
        results[model_name] = {
            'Precision': report['weighted avg']['precision'],
            'Recall': report['weighted avg']['recall'],
            'F1-Score': report['weighted avg']['f1-score'],
            'Accuracy': report['accuracy'],
            'Support': report['weighted avg']['support']
        }

    # Convert results to DataFrame for display
    df_results = pd.DataFrame(results).transpose()
    return df_results

# Train and evaluate models with chi-squared selected features
df_results_chi2 = train_and_evaluate_models(X_train_chi2_selected, X_test_chi2_selected)
print(df_results_chi2)

"""###**Naive Bayes, SVM, Random Forest, Decision Tree, Deep Neural Network (Model Ensembling - Bagging)**

**Description:**

This code segment is aimed at investigating the efficacy of ensemble learning, with a focus on bagging (Bootstrap Aggregating), in augmenting the document classification process. Bagging improves model robustness by training multiple model instances on various subsets of the training dataset and aggregating their outcomes, which can lead to reduced variance and bolstered generalization capabilities.

**Deep Neural Network Architecture:**

Utilizing the Keras library, a feed-forward neural network is devised, incorporating two hidden layers augmented with dropout layers to curtail the risk of overfitting. The network's output layer employs a softmax activation function, aligning with the requirements of multi-class classification challenges. The selected loss function, sparse_categorical_crossentropy, is optimized for targets encoded as integers.

**Bagging with Deep Neural Network:**

The ensemble is formed through the BaggingClassifier from the scikit-learn library, encapsulating the deep neural network previously established. This ensemble, comprising 10 neural network instances, each undergoes training on distinct subsets of the chi-squared feature-selected training data, aiming for a diverse learning experience across the ensemble.

**Consolidation of Results:**

Performance metrics from the deep neural network's bagging ensemble are synthesized alongside those from other models (e.g., SVM, Random Forest, Decision Tree, Naive Bayes), presuming these models have also been subjected to bagging techniques in earlier code sections. Key performance indicators, including precision, recall, F1-score, accuracy, and support, are utilized for an exhaustive evaluation of each model's effectiveness. These results are methodically arranged in a DataFrame, ensuring a straightforward and accessible presentation.

**Outcome:**
Executing this segment of code will unveil the performance metrics for each evaluated model under the application of bagging. This initiative aims to shed light on the potential benefits of ensemble learning in refining the robustness and generalization ability of document classification models, thereby enhancing their overall performance and reliability.
"""

# Model Definitions
def create_nn_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Model Training and Evaluation
def train_bagging_classifier(base_estimator, X_train, X_test, y_train, y_test):
    bagging_classifier = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)
    bagging_classifier.fit(X_train, y_train)
    y_pred = bagging_classifier.predict(X_test)
    report = classification_report(y_test, y_pred, output_dict=True)
    return {
        'Precision': report['weighted avg']['precision'],
        'Recall': report['weighted avg']['recall'],
        'F1-Score': report['weighted avg']['f1-score'],
        'Accuracy': report['accuracy'],
        'Support': report['weighted avg']['support']
    }

# Main Execution
results_bagging = {
    'Bagging SVM': train_bagging_classifier(SVC(kernel='linear'), X_train_chi2_selected, X_test_chi2_selected, y_train, y_test),
    'Bagging Random Forest': train_bagging_classifier(RandomForestClassifier(random_state=42), X_train_chi2_selected, X_test_chi2_selected, y_train, y_test),
    'Bagging Decision Tree': train_bagging_classifier(DecisionTreeClassifier(random_state=42), X_train_chi2_selected, X_test_chi2_selected, y_train, y_test),
    'Bagging Naive Bayes': train_bagging_classifier(MultinomialNB(), X_train_chi2_selected, X_test_chi2_selected, y_train, y_test),
    'Bagging Deep Neural Network': train_bagging_classifier(KerasClassifier(build_fn=lambda: create_nn_model(X_train_chi2_selected.shape[1], len(set(y_train))), epochs=10, batch_size=32, verbose=0), X_train_chi2_selected, X_test_chi2_selected, y_train, y_test)
}

# Convert results to DataFrame for display
df_results_bagging = pd.DataFrame(results_bagging).transpose()
print(df_results_bagging)

"""###**Naive Bayes, SVM, Random Forest, Decision Tree, Deep Neural Network (Regularization Techniques - Dropout)**

**Description:**

This code segment is meticulously designed to delve into the efficacy of various regularization techniques across different machine learning models to bolster document classification tasks. Regularization serves as a critical tool in mitigating overfitting, thus ensuring models maintain high performance on novel, unseen data.

**Deep Neural Network with Dropout Regularization:**

Utilizing the TensorFlow and Keras frameworks, a sophisticated feed-forward neural network is crafted. Dropout regularization is strategically implemented following dense layers, effectively randomizing a portion of the input units to zero during training iterations. This technique aids in curbing overfitting, enhancing the model's generalization capabilities. The network undergoes training utilizing chi-squared selected features from the training dataset, with subsequent evaluation performed on the test dataset.


**Support Vector Machines (SVM) with L1 and L2 Regularization:**

Two distinct SVM configurations are prepared: one augmented with L1 regularization and another with L2 regularization. L1 regularization is known for its capacity to induce sparsity within the weight vector, effectively performing feature selection. Conversely, L2 regularization works to prevent overfitting without driving weights to zero.

**Naive Bayes with Regularization:**

A Naive Bayes model is refined with Laplace smoothing, regulated through the alpha parameter. This form of regularization is pivotal in addressing the challenge of unseen features in the training dataset manifesting within the test data, thereby enhancing model robustness.

**Random Forest with Regularization:**

The Random Forest classifier is tailored with specific hyperparameters serving as regularization mechanisms. Parameters such as max_depth, which limits tree depth, and max_features, which dictates the subset of features considered for optimal splits, are crucial in preventing model overfitting.

The performance of each model, reflecting adjustments for regularization, is meticulously evaluated through metrics like precision, recall, F1-score, accuracy, and support. These insights are systematically aggregated and displayed within a DataFrame, offering a clear, comparative overview.

**Outcome:**
Executing this segment will unveil the performance metrics for each model, underscored by the application of distinct regularization strategies. This comparative analysis aims to shed light on the pivotal role of regularization in not only enhancing model fidelity on unseen datasets but also in improving overall model robustness and reliability.
"""

# Model Definitions
def create_nn_model(input_dim, num_classes):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dropout(0.5))  # Dropout layer for regularization
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))  # Dropout layer for regularization
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Model Training and Evaluation
def train_and_evaluate_classifier(classifier, X_train, X_test, y_train, y_test):
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    report = classification_report(y_test, y_pred, output_dict=True)
    return {
        'Precision': report['weighted avg']['precision'],
        'Recall': report['weighted avg']['recall'],
        'F1-Score': report['weighted avg']['f1-score'],
        'Accuracy': report['accuracy'],
        'Support': report['weighted avg']['support']
    }

# Main Execution
results_regularized = {
    'Dropout SVM L1': train_and_evaluate_classifier(LinearSVC(penalty='l1', dual=False, C=1.0), X_train_chi2_selected, X_test_chi2_selected, y_train, y_test),
    'Dropout SVM L2': train_and_evaluate_classifier(SVC(kernel='linear', C=1.0), X_train_chi2_selected, X_test_chi2_selected, y_train, y_test),
    'Dropout Naive Bayes Regularized': train_and_evaluate_classifier(MultinomialNB(alpha=0.5), X_train_chi2_selected, X_test_chi2_selected, y_train, y_test),
    'Dropout Random Forest Regularized': train_and_evaluate_classifier(RandomForestClassifier(n_estimators=100, max_depth=10, max_features='sqrt', random_state=42), X_train_chi2_selected, X_test_chi2_selected, y_train, y_test),
    'Dropout Decision Tree': train_and_evaluate_classifier(DecisionTreeClassifier(random_state=42), X_train_chi2_selected, X_test_chi2_selected, y_train, y_test),
    'Dropout Deep Neural Network': train_and_evaluate_classifier(KerasClassifier(build_fn=lambda: create_nn_model(X_train_chi2_selected.shape[1], len(set(y_train))), epochs=10, batch_size=32, verbose=0), X_train_chi2_selected, X_test_chi2_selected, y_train, y_test)
}

# Convert results to DataFrame for display
df_results_regularized = pd.DataFrame(results_regularized).transpose()
print(df_results_regularized)

"""##1.3. Models' Evaluation

**Objective and Results Consolidation:**

This segment of code is meticulously crafted with the aim of amalgamating the outcomes derived from a variety of model setups and preprocessing strategies. The spectrum of models encompasses baseline models, models augmented with N-grams, those utilizing Word2Vec embeddings, models refined through chi-squared feature selection, ensembles created via bagging, and models fortified with regularization techniques. By aggregating these results into a singular DataFrame, we achieve a comprehensive overview, facilitating an efficient comparative analysis of diverse model performances.

**F1-Score Based Sorting for Optimal Model Identification:**

To distill the essence of model efficacy, the assembled results are systematically arranged in descending order, prioritizing the F1-Score. This metric, serving as the harmonic mean of precision and recall, stands as a pivotal benchmark for gauging model precision and sensitivity, thereby ensuring a balanced evaluation especially in scenarios of uneven class distribution.

**Results Display and Model Ascertainment:**

The methodically sorted results are then elucidated in a tabular representation, ensuring clarity and ease of interpretation regarding the performance hierarchy of the model configurations.

**Identification of the Best Model:**

In this analytical process, the spotlight shines on identifying the model that stands out with the highest F1-Score. This model is duly recognized as the most proficient, with specific emphasis on its configuration and the achieved F1-Score, offering tangible insights into its superior performance.

**Outcome:**

Following the execution of this code segment, a detailed exposition of the performance metrics across all considered model configurations will be unveiled. This initiative not only underscores the model that exemplifies the best performance based on the F1-Score but also furnishes a definitive guide on the most efficacious model configuration for the task at hand, informed by empirical evidence.
"""

# Consolidate all results
all_results = pd.concat([
    df_results,
    df_results_ngrams,
    df_results_word2vec,
    df_results_chi2,
    df_results_bagging,
    df_results_regularized,
])

# Sort based on F1-Score
sorted_results = all_results.sort_values(by='F1-Score', ascending=False)

# Display the consolidated results
print(sorted_results)

# Display the best model
best_model = sorted_results.iloc[0]
print("\nBest Model based on F1-Score:")
print(best_model.name)
print("F1-Score:", best_model['F1-Score'])

"""**F1-Score Based  Based Sorting for Optimal Model Identification Grouped by Enhancement Technique:**

To distill the essence of model efficacy based on F1-Score with the different enhancement techniques, the resulted are filtered based on F1-Score and grouped in enhancement techniques groups. This metric, serving as the harmonic mean of precision and recall, stands as a pivotal benchmark for gauging model precision and sensitivity, thereby ensuring a balanced evaluation especially in scenarios of uneven class distribution.

"""

# Assuming the first unique word in the index represents the model name
# Extract model names from the index (or a specific column if the index was reset)
all_results['Model'] = all_results.index.to_series().apply(lambda x: x.split()[0])

# Now that we have a 'Model' column, we can filter the results for each unique model name
unique_models = all_results['Model'].unique()

# Display the results for each unique model
for model in unique_models:
    print(f"Results for {model}:")
    model_results = all_results[all_results['Model'] == model]
    print(model_results.drop('Model', axis=1))  # Drop the 'Model' column for display
    print("\n")  # Add a newline for readability

"""**Analytical Examination of Model Enhancements Using ANOVA:**

This code segment embarks on a statistical exploration to discern the effectiveness of various enhancement techniques applied to document classification models. By leveraging the Analysis of Variance (ANOVA), the code aims to identify statistically significant differences in performance, as measured by the F1-Score, across different model configurations.

**Dataframe Preparation and Initial Setup:**

The initial step involves ensuring that the 'all_results' DataFrame is aptly structured, containing essential columns such as 'Model' and 'F1-Score'. The 'Model' column, which is pivotal for this analysis, is derived from the DataFrame's index, as has been delineated in prior discussions.

**Statistical Analysis via ANOVA:**

Subsequently, the ANOVA test is conducted on the F1-Scores derived from different enhancement techniques. This statistical method allows for the comparison of means across multiple groups, in this case, the unique models, to ascertain if the observed differences in F1-Scores are statistically significant beyond chance. The computation is facilitated through the 'f_oneway' function from the 'stats' library, iterating over the F1-Scores for each model category within the 'all_results' DataFrame.

**Interpreting the ANOVA Results:**

Upon completion of the ANOVA test, the results are presented, highlighting the F-statistic and the corresponding P-value. The F-statistic gives an indication of the ratio of variance between the model groups to the variance within the groups, while the P-value assists in determining the statistical significance of the observed variance.

**Decision Criteria Based on P-value:**

A critical decision threshold is set at a P-value of less than 0.05. Should the ANOVA test result in a P-value below this threshold, it is inferred that there exists a statistically significant difference in the F1-Scores across the models. This would suggest that at least one model's performance is distinctly different, attributed to the enhancement techniques employed. Conversely, a P-value exceeding 0.05 would indicate no significant variance among the models' F1-Scores, implying that the enhancement techniques did not lead to markedly different outcomes in model performance.

**Outcome and Implications:**

The execution of this code segment not only sheds light on the statistical significance of the differences in F1-Scores across various model enhancements but also guides future decisions on model selection and improvement strategies. By understanding which enhancements yield statistically significant improvements, efforts can be more effectively channeled towards optimizing model performance in document classification tasks.







"""

# Ensure that 'all_results' DataFrame has 'Model' and 'F1-Score' columns
# 'Model' column can be created from the index as previously discussed

# Perform ANOVA on the F1-Score across different enhancment tecniques
anova_results = stats.f_oneway(*(all_results[all_results['Model'] == model]['F1-Score'] for model in unique_models))

print("ANOVA results:")
print(f"F-statistic: {anova_results.statistic}")
print(f"P-value: {anova_results.pvalue}")

# If the p-value is less than 0.05, we reject the null hypothesis and infer that
# there are significant differences between the F1 scores of the models
if anova_results.pvalue < 0.05:
    print("At least one model's F1 score is significantly different.")
else:
    print("No significant difference found between the enhancment tecniques' F1 scores.")

"""# **2. Expiriement with BBC Full Text Document Classification Dataset (DATASET 2)**

## 2.1. Data Uploading and Data Preprocessing

Firstly it is necessary to upload the file in zip where many different folders with different categories of files are holded, so to unpack it for the further experiments.
"""

# Once uploaded, unzip using:
!unzip bbc.zip
!ls

"""Following the initual unzipping process, it is extracted each category, and setting 30% of data for training and 70% for testing."""

# Define the base path where the 'bbc' folder is located
# Make sure this path points directly to the folder containing the category folders.
base_path = Path('bbc')  # Replace with the correct path to your 'bbc' folder

# Dynamically list categories based on the directories in the 'bbc' folder
categories = [f.name for f in base_path.iterdir() if f.is_dir()]

texts = []
labels = []

print(f"Found categories: {categories}")

# Iterate over the categories and read the files
for category in categories:
    category_path = base_path / category
    # Print the current category path for debugging
    print(f"Reading files from category: {category}")

    for file_path in category_path.iterdir():
        if file_path.is_file() and not file_path.name.startswith('.'):
            # Print each file path being read for debugging
            print(f"Reading file: {file_path}")

            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                texts.append(file.read())
                labels.append(category)

# Check if we have collected any texts and labels
if not texts or not labels:
    raise ValueError("No texts or labels have been collected. Please check your directory paths and file permissions.")

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)

"""##2.2. ML Models Creation and Execution of the Experiment

###**Naive Bayes, SVM, Random Forest, Decision Tree, Deep Neural Network (No enhancment tecniques)**

**Description:**

This segment of code is meticulously designed to refine the process of classifying documents by integrating the concept of bigrams (sequential pairs of words) and conducting a thorough evaluation of a variety of machine learning algorithms, including an advanced deep neural network.

**Bigram Incorporation:**
Utilizing the TfidfVectorizer with the ngram_range parameter configured to (1,1). This is the standard way, and so considered to be without any enhancements.For seamless integration with Keras, the datasets designated for training and testing are transformed through this vectorization method and subsequently converted into array formats.


**Deep Neural Network Definition:**

Employing the Keras toolkit, a feed-forward neural network is constructed featuring two intermediary layers enhanced with dropout regularization to mitigate the risk of overfitting. The terminal layer is activated using a softmax function, making it apt for addressing multi-category classification challenges. The chosen loss function is sparse_categorical_crossentropy, deemed suitable for dealing with class labels encoded as integers.


**Model Training and Evaluation:**

The study delineates four distinct models for evaluation: Naive Bayes, Support Vector Machine (SVM), Random Forest, Decision Tree and the deep neural network as previously described. Each model undergoes training utilizing the training dataset and is subsequently assessed on the test dataset.

The evaluation of each model's efficacy is conducted through a comprehensive set of metrics, including precision, recall, F1-score, accuracy, and support, offering a holistic view of the performance across both positive and negative classifications.

The outcomes of these evaluations are methodically compiled and exhibited in a tabular arrangement via a DataFrame.

**Outcome:**

Executing this code will reveal the performance metrics for each model, highlighting the advantage of incorporating bigrams (1,1) in the feature extraction process. Besides, that results will be holded as the benchmarks for the future experiments with DATASET 1.
"""

# Using only unigrams
tfidf_vectorizer_ngrams = TfidfVectorizer(stop_words='english', ngram_range=(1,1))
X_train_tfidf_ngrams = tfidf_vectorizer_ngrams.fit_transform(X_train).toarray()  # Convert to array for Keras
X_test_tfidf_ngrams = tfidf_vectorizer_ngrams.transform(X_test).toarray()

# Define a simple feed-forward neural network for Keras
def create_nn_model():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train_tfidf_ngrams.shape[1], activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def train_and_evaluate_models(X_train, X_test):
    # Define the models
    models = {
        'Naive Bayes': MultinomialNB(),
        'SVM': SVC(kernel='linear'),
        'Random Forest': RandomForestClassifier(random_state=42),
        'Decision Tree': DecisionTreeClassifier(random_state=42),
        'Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)
    }

    # Train and evaluate the models
    results = {}
    for model_name, model in models.items():
        # Train
        model.fit(X_train, y_train)
        # Predict
        y_pred = model.predict(X_test)
        # Evaluate
        report = classification_report(y_test, y_pred, output_dict=True)
        results[model_name] = {
            'Precision': report['weighted avg']['precision'],
            'Recall': report['weighted avg']['recall'],
            'F1-Score': report['weighted avg']['f1-score'],
            'Accuracy': report['accuracy'],
            'Support': report['weighted avg']['support']
        }

    # Convert results to DataFrame for display
    df_results = pd.DataFrame(results).transpose()
    return df_results

# Train and evaluate models with Unigrams
df_results = train_and_evaluate_models(X_train_tfidf_ngrams, X_test_tfidf_ngrams)
print(df_results)

"""###**Naive Bayes, SVM, Random Forest, Decision Tree, Deep Neural Network (Feature Engineering - N-grams)**

**Description:**

This code segment focuses on advancing document classification techniques, with a special emphasis on the utilization of bigrams (consecutive word pairs) as critical features. It conducts a thorough evaluation of a variety of machine learning methodologies, including an intricate deep neural network.

**Bigram Feature Extraction:**

Through the application of the TfidfVectorizer, configured with an ngram_range parameter of (1,2), this technique embraces a comprehensive vectorization strategy. It accounts for both singular words (unigrams) and their adjacent couplings (bigrams), aiming to seize a deeper contextual understanding beyond the scope of unigrams alone. The datasets designated for training and testing are processed with this advanced vectorization, then converted into array formats to ensure they mesh seamlessly with the Keras environment.

**Deep Neural Network Architecture:**

Leveraging the capabilities of the Keras framework, a sophisticated feed-forward neural network is crafted. This network features two internal layers, punctuated by dropout layers to counteract overfitting effectively. Its output layer employs a softmax activation function, ideally suited for the complexities of multi-class classification tasks. The selection of sparse_categorical_crossentropy as the loss function aligns perfectly with the needs of integer-encoded class labels.

**Model Training & Evaluation Framework:**

A quartet of models is elaborated upon: Naive Bayes, Support Vector Machine (SVM), Random Forest, Decision Tree and the deep neural network as previously articulated. Training these models with the provided dataset precedes a meticulous evaluation on the testing dataset.

To assess the performance of each model accurately, a suite of metrics—precision, recall, F1-score, accuracy, and support—is utilized. These measures afford a detailed perspective on the models' effectiveness, covering both positive and negative class predictions. The synthesized results are systematically organized and displayed in a DataFrame, offering a clear, tabular overview.

**Outcome:**

The execution of this code is poised to unveil the performance metrics for each model, underlining the significant benefits of integrating bigrams into the feature extraction phase. This demonstration aims to highlight the value of incorporating richer contextual data from text, potentially elevating the efficacy of document classification endeavors.
"""

# Using bigrams
tfidf_vectorizer_ngrams = TfidfVectorizer(stop_words='english', ngram_range=(1,2))
X_train_tfidf_ngrams = tfidf_vectorizer_ngrams.fit_transform(X_train).toarray()  # Convert to array for Keras
X_test_tfidf_ngrams = tfidf_vectorizer_ngrams.transform(X_test).toarray()

# Define a simple feed-forward neural network for Keras
def create_nn_model():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train_tfidf_ngrams.shape[1], activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def train_and_evaluate_models(X_train, X_test):
    # Define the models
    models = {
        'N-grams Naive Bayes': MultinomialNB(),
        'N-grams SVM': SVC(kernel='linear'),
        'N-grams Random Forest': RandomForestClassifier(random_state=42),
        'N-grams Decision Tree': DecisionTreeClassifier(random_state=42),
        'N-grams Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)
    }

    # Train and evaluate the models
    results = {}
    for model_name, model in models.items():
        # Train
        model.fit(X_train, y_train)
        # Predict
        y_pred = model.predict(X_test)
        # Evaluate
        report = classification_report(y_test, y_pred, output_dict=True)
        results[model_name] = {
            'Precision': report['weighted avg']['precision'],
            'Recall': report['weighted avg']['recall'],
            'F1-Score': report['weighted avg']['f1-score'],
            'Accuracy': report['accuracy'],
            'Support': report['weighted avg']['support']
        }

    # Convert results to DataFrame for display
    df_results = pd.DataFrame(results).transpose()
    return df_results

# Train and evaluate models with N-grams
df_results_ngrams = train_and_evaluate_models(X_train_tfidf_ngrams, X_test_tfidf_ngrams)
print(df_results_ngrams)

"""###**Naive Bayes, SVM, Random Forest, Decision Tree, Deep Neural Network (Word Embeddings - Pre-trained Embeddings)**

**Description:**

This code segment is ingeniously crafted for the purpose of document classification, harnessing the sophisticated capabilities of Word2Vec embeddings. Word2Vec, a cutting-edge pre-trained model, excels in capturing the semantic intricacies between words by mapping them as vectors within a vast, high-dimensional space. The effectiveness of a variety of machine learning models, including a sophisticated deep neural network, is meticulously evaluated using these semantic embeddings.

**Word2Vec Embeddings:**

The renowned Google News Word2Vec model, distinguished by its training on an extensive corpus and boasting a vector size of 300, is adeptly loaded.
A specialized function, document_to_word2vec, is crafted to transform a document into its Word2Vec vector representation by averaging the vectors of the words it contains. This transformation process is applied to both the training and testing datasets, converting them into Word2Vec representations for further processing.

**Deep Neural Network Architecture:**

Utilizing the versatile Keras library, a robust feed-forward neural network is developed, featuring two hidden layers. These layers are interspersed with dropout layers to effectively curtail the risk of overfitting. The activation function selected for the output layer is softmax, rendering it ideal for multi-class classification endeavors. The loss function of choice, sparse_categorical_crossentropy, is particularly well-suited for handling integer-encoded class labels.


**Model Training & Evaluation Framework:**

The evaluation framework encompasses four distinct models: Naive Bayes (specifically GaussianNB, tailored for the continuous nature of Word2Vec features), Support Vector Machine (SVM), Random Forest, Decision Tree and the aforementioned Deep Neural Network.

Training is conducted on the datasets transformed via Word2Vec, followed by a thorough evaluation on the test dataset.To comprehensively assess the performance of each model, a variety of metrics including precision, recall, F1-score, accuracy, and support are utilized. These metrics offer a well-rounded perspective on each model's performance, accounting for both positive and negative class outcomes.The evaluation results are methodically compiled and displayed in a structured DataFrame.

**Outcome:**
Upon execution, this code segment is designed to unveil the performance metrics for each evaluated model, leveraging the sophisticated Word2Vec embeddings as features. This initiative aims to highlight the significant benefits of integrating semantic understanding into the textual data analysis, thereby enhancing the document classification process.
"""

def document_to_word2vec(doc, model):
    # Tokenize the document, filter out words not in the model's vocabulary
    words = [word for word in doc.split() if word in model.key_to_index]
    if len(words) == 0:
        return np.zeros(model.vector_size)
    # Convert words to vectors and average them
    return np.mean([model[word] for word in words], axis=0)

X_train_word2vec = np.array([document_to_word2vec(doc, word2vec_model) for doc in X_train])
X_test_word2vec = np.array([document_to_word2vec(doc, word2vec_model) for doc in X_test])

# Define a simple feed-forward neural network for Keras
def create_nn_model():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train_word2vec.shape[1], activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def train_and_evaluate_models(X_train, X_test):
    # Define the models
    models = {
        'Pre-trained Embeddings Naive Bayes': GaussianNB(),
        'Pre-trained Embeddings SVM': SVC(kernel='linear'),
        'Pre-trained Embeddings Random Forest': RandomForestClassifier(random_state=42),
        'Pre-trained Embeddings Decision Tree': DecisionTreeClassifier(random_state=42),
        'Pre-trained Embeddings Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)
    }

    # Train and evaluate the models
    results = {}
    for model_name, model in models.items():
        # Train
        model.fit(X_train, y_train)
        # Predict
        y_pred = model.predict(X_test)
        # Evaluate
        report = classification_report(y_test, y_pred, output_dict=True)
        results[model_name] = {
            'Precision': report['weighted avg']['precision'],
            'Recall': report['weighted avg']['recall'],
            'F1-Score': report['weighted avg']['f1-score'],
            'Accuracy': report['accuracy'],
            'Support': report['weighted avg']['support']
        }

    # Convert results to DataFrame for display
    df_results = pd.DataFrame(results).transpose()
    return df_results

# Train and evaluate models with Word2Vec features
df_results_word2vec = train_and_evaluate_models(X_train_word2vec, X_test_word2vec)
print(df_results_word2vec)

"""###**Naive Bayes, SVM, Random Forest, Decision Tree, Deep Neural Network (Feature Selection - Chi-Squared Test)**

**Description:**

This code segment is engineered to advance document classification efforts by integrating bigrams and utilizing the chi-squared test for feature selection. The aim is to explore how these methodologies affect the performance of a variety of machine learning models, including an advanced deep neural network.

**Bigrams with TF-IDF:**

Employing the TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer, this approach is designed to acknowledge both single words (unigrams) and word pairs (bigrams), enabling the extraction of richer contextual insights that could enhance model comprehension of textual content. The datasets for training and testing are accordingly transformed into TF-IDF representations and then arrayed to ensure seamless integration with the Keras framework.

**Feature Selection using Chi-Squared Test:**

The chi-squared test, a method to assess the independence of two categorical variables, is applied to pinpoint the top 10,000 features demonstrating the most significant associations with the target variable. This process refines the TF-IDF transformed datasets by concentrating on these key features, potentially bolstering model performance by focusing on the most relevant data.

**Deep Neural Network Architecture:**

A sophisticated feed-forward neural network is established using Keras, featuring two intermediate layers accompanied by dropout mechanisms to curtail overfitting risks. The network employs a softmax activation function for the output layer, aligning with the demands of multi-class classification tasks. The loss function selected, sparse_categorical_crossentropy, is tailored for handling integer-encoded target labels.

**Model Training & Evaluation Framework:**

The approach delineates four specific models for assessment: Naive Bayes, Support Vector Machine (SVM), Random Forest, Decision Tree and the deep neural network previously outlined. Each model is trained with the chi-squared test refined features from the training dataset and evaluated against the test dataset.

The effectiveness of each model is measured using key performance metrics such as precision, recall, F1-score, accuracy, and support. These metrics offer a rounded evaluation of performance across both the positive and negative classification outcomes. The findings are systematically assembled and displayed in a DataFrame for easy comparison.

**Outcome:**
Executing this segment of code will showcase the performance metrics for each model, highlighting the efficacy of employing bigrams alongside chi-squared feature selection. This execution is intended to illuminate the benefits of these sophisticated techniques in deepening the model's grasp of textual nuances.
"""

# Using bigrams
tfidf_vectorizer_ngrams = TfidfVectorizer(stop_words='english', ngram_range=(1,2))
X_train_tfidf_ngrams = tfidf_vectorizer_ngrams.fit_transform(X_train).toarray()  # Convert to array for Keras
X_test_tfidf_ngrams = tfidf_vectorizer_ngrams.transform(X_test).toarray()

# Select top 10,000 features based on the chi-squared test
k_best = 10000
ch2 = SelectKBest(chi2, k=k_best)
X_train_chi2_selected = ch2.fit_transform(X_train_tfidf_ngrams, y_train)
X_test_chi2_selected = ch2.transform(X_test_tfidf_ngrams)

# Define a simple feed-forward neural network for Keras
def create_nn_model():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train_chi2_selected.shape[1], activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(len(set(y_train)), activation='softmax'))  # Number of classes
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def train_and_evaluate_models(X_train, X_test):
    # Define the models
    models = {
        'Chi-Squared Test Naive Bayes': MultinomialNB(),
        'Chi-Squared Test SVM': SVC(kernel='linear'),
        'Chi-Squared Test Random Forest': RandomForestClassifier(random_state=42),
        'Chi-Squared Test Decision Tree': DecisionTreeClassifier(random_state=42),
        'Chi-Squared Test Deep Neural Network': KerasClassifier(build_fn=create_nn_model, epochs=10, batch_size=32, verbose=0)
    }

    # Train and evaluate the models
    results = {}
    for model_name, model in models.items():
        # Train
        model.fit(X_train, y_train)
        # Predict
        y_pred = model.predict(X_test)
        # Evaluate
        report = classification_report(y_test, y_pred, output_dict=True)
        results[model_name] = {
            'Precision': report['weighted avg']['precision'],
            'Recall': report['weighted avg']['recall'],
            'F1-Score': report['weighted avg']['f1-score'],
            'Accuracy': report['accuracy'],
            'Support': report['weighted avg']['support']
        }

    # Convert results to DataFrame for display
    df_results = pd.DataFrame(results).transpose()
    return df_results

# Train and evaluate models with chi-squared selected features
df_results_chi2 = train_and_evaluate_models(X_train_chi2_selected, X_test_chi2_selected)
print(df_results_chi2)

"""###**Naive Bayes, SVM, Random Forest, Decision Tree, Deep Neural Network (Model Ensembling - Bagging)**

##2.3. Models' Evaluation

**Objective and Results Consolidation:**

This segment of code is meticulously crafted with the aim of amalgamating the outcomes derived from a variety of model setups and preprocessing strategies. The spectrum of models encompasses baseline models, models augmented with N-grams, those utilizing Word2Vec embeddings, models refined through chi-squared feature selection, ensembles created via bagging, and models fortified with regularization techniques. By aggregating these results into a singular DataFrame, we achieve a comprehensive overview, facilitating an efficient comparative analysis of diverse model performances.

**F1-Score Based Sorting for Optimal Model Identification:**

To distill the essence of model efficacy, the assembled results are systematically arranged in descending order, prioritizing the F1-Score. This metric, serving as the harmonic mean of precision and recall, stands as a pivotal benchmark for gauging model precision and sensitivity, thereby ensuring a balanced evaluation especially in scenarios of uneven class distribution.

**Results Display and Model Ascertainment:**

The methodically sorted results are then elucidated in a tabular representation, ensuring clarity and ease of interpretation regarding the performance hierarchy of the model configurations.

**Identification of the Best Model:**

In this analytical process, the spotlight shines on identifying the model that stands out with the highest F1-Score. This model is duly recognized as the most proficient, with specific emphasis on its configuration and the achieved F1-Score, offering tangible insights into its superior performance.

**Outcome:**

Following the execution of this code segment, a detailed exposition of the performance metrics across all considered model configurations will be unveiled. This initiative not only underscores the model that exemplifies the best performance based on the F1-Score but also furnishes a definitive guide on the most efficacious model configuration for the task at hand, informed by empirical evidence.
"""

# Consolidate all results
all_results = pd.concat([
    df_results,
    df_results_ngrams,
    df_results_word2vec,
    df_results_chi2,
    df_results_bagging,
    df_results_regularized,
])

# Sort based on F1-Score
sorted_results = all_results.sort_values(by='F1-Score', ascending=False)

# Display the consolidated results
print(sorted_results)

# Display the best model
best_model = sorted_results.iloc[0]
print("\nBest Model based on F1-Score:")
print(best_model.name)
print("F1-Score:", best_model['F1-Score'])

"""**F1-Score Based  Based Sorting for Optimal Model Identification Grouped by Enhancement Technique:**

To distill the essence of model efficacy based on F1-Score with the different enhancement techniques, the resulted are filtered based on F1-Score and grouped in enhancement techniques groups. This metric, serving as the harmonic mean of precision and recall, stands as a pivotal benchmark for gauging model precision and sensitivity, thereby ensuring a balanced evaluation especially in scenarios of uneven class distribution.

"""

# Assuming the first unique word in the index represents the model name
# Extract model names from the index (or a specific column if the index was reset)
all_results['Model'] = all_results.index.to_series().apply(lambda x: x.split()[0])

# Now that we have a 'Model' column, we can filter the results for each unique model name
unique_models = all_results['Model'].unique()

# Display the results for each unique model
for model in unique_models:
    print(f"Results for {model}:")
    model_results = all_results[all_results['Model'] == model]
    print(model_results.drop('Model', axis=1))  # Drop the 'Model' column for display
    print("\n")  # Add a newline for readability

"""**Analytical Examination of Model Enhancements Using ANOVA:**

This code segment embarks on a statistical exploration to discern the effectiveness of various enhancement techniques applied to document classification models. By leveraging the Analysis of Variance (ANOVA), the code aims to identify statistically significant differences in performance, as measured by the F1-Score, across different model configurations.

**Dataframe Preparation and Initial Setup:**

The initial step involves ensuring that the 'all_results' DataFrame is aptly structured, containing essential columns such as 'Model' and 'F1-Score'. The 'Model' column, which is pivotal for this analysis, is derived from the DataFrame's index, as has been delineated in prior discussions.

**Statistical Analysis via ANOVA:**

Subsequently, the ANOVA test is conducted on the F1-Scores derived from different enhancement techniques. This statistical method allows for the comparison of means across multiple groups, in this case, the unique models, to ascertain if the observed differences in F1-Scores are statistically significant beyond chance. The computation is facilitated through the 'f_oneway' function from the 'stats' library, iterating over the F1-Scores for each model category within the 'all_results' DataFrame.

**Interpreting the ANOVA Results:**

Upon completion of the ANOVA test, the results are presented, highlighting the F-statistic and the corresponding P-value. The F-statistic gives an indication of the ratio of variance between the model groups to the variance within the groups, while the P-value assists in determining the statistical significance of the observed variance.

**Decision Criteria Based on P-value:**

A critical decision threshold is set at a P-value of less than 0.05. Should the ANOVA test result in a P-value below this threshold, it is inferred that there exists a statistically significant difference in the F1-Scores across the models. This would suggest that at least one model's performance is distinctly different, attributed to the enhancement techniques employed. Conversely, a P-value exceeding 0.05 would indicate no significant variance among the models' F1-Scores, implying that the enhancement techniques did not lead to markedly different outcomes in model performance.

**Outcome and Implications:**

The execution of this code segment not only sheds light on the statistical significance of the differences in F1-Scores across various model enhancements but also guides future decisions on model selection and improvement strategies. By understanding which enhancements yield statistically significant improvements, efforts can be more effectively channeled towards optimizing model performance in document classification tasks.
"""

# Ensure that 'all_results' DataFrame has 'Model' and 'F1-Score' columns
# 'Model' column can be created from the index as previously discussed

# Perform ANOVA on the F1-Score across different enhancment tecniques
anova_results = stats.f_oneway(*(all_results[all_results['Model'] == model]['F1-Score'] for model in unique_models))

print("ANOVA results:")
print(f"F-statistic: {anova_results.statistic}")
print(f"P-value: {anova_results.pvalue}")

# If the p-value is less than 0.05, we reject the null hypothesis and infer that
# there are significant differences between the F1 scores of the models
if anova_results.pvalue < 0.05:
    print("At least one model's F1 score is significantly different.")
else:
    print("No significant difference found between the enhancment tecniques' F1 scores.")

"""# **3. SVM ML Extraction For App Creation**

The findings from our investigation reveal that the Support Vector Machine (SVM) machine learning model, even without the application of enhancement techniques, stands out for its computational efficiency and robust average performance metrics. In light of these results, we opted for a merged dataset approach, amalgamating data from DATASET 1 and DATASET 2. This strategy not only enriches our training corpus with a diverse array of texts spanning business, entertainment, politics, sports, and technology but also closely aligns with our client's requirements for the final application deployment. Consequently, this section delineates the process for extracting and externalizing the SVM model, thereby facilitating its integration and application in the deployment of our app.

## 3.1 Data Uploading and Data Preprocessing

Firstly it is necessary to upload the file in zip where many different folders with different categories of files are holded, so to unpack it for the further experiments.
"""

# Once uploaded, unzip using:
!unzip Data_For_the_App_Training.zip
!ls

"""Following the initual unzipping process, it is extracted each category, and setting 30% of data for training and 70% for testing."""

# Define the base path to your dataset
base_path = Path('Data_For_the_App_Training/Data')  # Adjust this to the path of your dataset

# Dynamically list categories based on directories in the dataset folder
categories = [f.name for f in base_path.iterdir() if f.is_dir()]
texts = []
labels = []

# Iterate over categories and read files
for category in categories:
    category_path = base_path / category
    for file_path in category_path.iterdir():
        if file_path.is_file() and not file_path.name.startswith('.'):
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                texts.append(file.read())
                labels.append(category)

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)

"""##3.2. SVM ML Model Creation For the App

After the data preprocessing, code snippet trains and evaluates a Support Vector Machine (SVM) model for text classification. It starts by converting the training and testing text data into TF-IDF features, excluding English stop words and using unigrams. An SVM model with a linear kernel is then initialized and trained on the transformed training data. Finally, the model's performance is evaluated on the test set, with the results displayed through a classification report that includes metrics such as precision, recall, and F1-score.
"""

# Training the SVM model
tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Initialize SVM with probability estimates
svm_model = SVC(kernel='linear', probability=True)
svm_model.fit(X_train_tfidf, y_train)

# Evaluating the model
y_pred = svm_model.predict(X_test_tfidf)
print(classification_report(y_test, y_pred))

"""This code snippet saves the trained SVM model and the TF-IDF vectorizer to disk using Python's pickle module. It serializes the objects into binary format and writes them to files named svm_model.pkl and tfidf_vectorizer.pkl, respectively. This allows for the model and vectorizer to be easily reloaded for future predictions without needing to retrain or refit.

"""

# Serialize and save the SVM model
with open('svm_model.pkl', 'wb') as file:
    pickle.dump(svm_model, file)

# Serialize and save the TF-IDF vectorizer
with open('tfidf_vectorizer.pkl', 'wb') as file:
    pickle.dump(tfidf_vectorizer, file)

"""This code triggers the download of the serialized TF-IDF vectorizer and SVM model files (tfidf_vectorizer.pkl and svm_model.pkl) from the server to your local machine, enabling the use of the trained model and vectorizer in the online app."""

files.download('tfidf_vectorizer.pkl')
files.download('svm_model.pkl')